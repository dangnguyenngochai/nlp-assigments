{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for dataset preparation, feature engineering\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.random.seed(123) #for reprodicible results\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Dataset preparation ##\n",
    "\n",
    "The dataset is collected from this kaggle compettion: https://www.kaggle.com/c/learn-ai-bbc/data\n",
    "\n",
    "The dataset contains 1117 news article from the BBC News and each of them are classified into one of the following categories: bussiness, tech, politics, sport, entertainment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bbc_news_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business' 'tech' 'politics' 'sport' 'entertainment']\n",
      "Number of business documents: 336\n",
      "Number of tech documents: 261\n",
      "Number of politics documents: 274\n",
      "Number of sport documents: 346\n",
      "Number of entertainment documents: 273\n"
     ]
    }
   ],
   "source": [
    "labels_num = df.Category.unique()\n",
    "print(labels_num)\n",
    "business = df[df['Category'] == 'business'].shape[0]\n",
    "tech = df[df['Category'] == 'tech'].shape[0]\n",
    "politics = df[df['Category'] == 'politics'].shape[0]\n",
    "sport = df[df['Category'] == 'sport'].shape[0]\n",
    "entertainment = df[df['Category'] == 'entertainment'].shape[0]\n",
    "print('Number of business documents: %s' %business)\n",
    "print('Number of tech documents: %s' %tech)\n",
    "print('Number of politics documents: %s' %politics)\n",
    "print('Number of sport documents: %s' %sport)\n",
    "print('Number of entertainment documents: %s' %entertainment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'howard  truanted to play snooker  conservative leader michael howard has admitted he used to play truant to spend time with his school friends at a snooker hall.  mr howard said his time at jack s snooker hall in llanelli in the 1950s had not done him  any lasting damage . but he told the times educational supplement that truancy was  very bad  and said  firm action  was needed. mr howard also called for a return to o-levels and more classroom discipline.  mr howard eventually left llanelli grammar school - and the snooker hall - to go to cambridge university. he said:  i don t think it s done me any lasting damage. nor has it made me a snooker world champion.  there might have been some occasions when we left early of an afternoon.   i m just being honest. i think truancy is a very bad thing and that firm action should be taken to deal with it.  another player who has failed to win snooker s world championship - jimmy  the whirlwind   white - has previously admitted missing lessons  instead spending his days in smoky halls.  tony meo [another player] and me used to spend all of our spare time there   mr white said   we loved the game and the atmosphere.  school went out of the window. i went for a while and then started taking time off.  mr howard s fellow welshman ray reardon - known by his fellow professionals as  dracula  - won the snooker world championship six times  having left school at 14 to work as a miner. and terry griffiths  like mr howard from llanelli  won the tournament in 1979. it is not known whether the two of them ever clashed cues at jack s.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split the dataset into training and test sets so that we can train and test classifier. Also, we will encode our target column so that it can be used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test datasets \n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df['Text'], df['Category'])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y.values)\n",
    "test_y = encoder.fit_transform(test_y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectors as features\n",
    "TF-IDF score represents the relative importance of a term in the document and the entire corpus. TF-IDF score is composed by two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['Text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training documents: 1117\n",
      "Number of testing documents: 373\n",
      "Number of features of each document: 5000\n",
      "xtrain_tfidf shape: (1117, 5000)\n",
      "train_y shape: (1117,)\n",
      "xtest_tfidf shape: (373, 5000)\n",
      "test_y shape: (373,)\n"
     ]
    }
   ],
   "source": [
    "# Getting transformed training and testing dataset\n",
    "print('Number of training documents: %s' %str(xtrain_tfidf.shape[0]))\n",
    "print('Number of testing documents: %s' %str(xtest_tfidf.shape[0]))\n",
    "print('Number of features of each document: %s' %str(xtrain_tfidf.shape[1]))\n",
    "print('xtrain_tfidf shape: %s' %str(xtrain_tfidf.shape))\n",
    "print('train_y shape: %s' %str(train_y.shape))\n",
    "print('xtest_tfidf shape: %s' %str(xtest_tfidf.shape))\n",
    "print('test_y shape: %s' %str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02075747, 0.        , 0.02915101, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "train_y = np.expand_dims(train_y, axis=0)\n",
    "test_y = np.expand_dims(test_y, axis=0)\n",
    "\n",
    "# for convenience in this exercise, we also use toarray() to convert sparse to dense matrix \n",
    "xtrain_tfidf =  xtrain_tfidf.T.toarray() \n",
    "xtest_tfidf =  xtest_tfidf.T.toarray()\n",
    "### END CODE HERE ###\n",
    "xtrain_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain_tfidf shape: (5000, 1117)\n",
      "train_y shape: (1, 1117)\n",
      "xtest_tfidf shape: (5000, 373)\n",
      "test_y shape: (1, 373)\n"
     ]
    }
   ],
   "source": [
    "# New shape \n",
    "print('xtrain_tfidf shape: %s' %str(xtrain_tfidf.shape))\n",
    "print('train_y shape: %s' %str(train_y.shape))\n",
    "print('xtest_tfidf shape: %s' %str(xtest_tfidf.shape))\n",
    "print('test_y shape: %s' %str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(input_num, output_num=1, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    This function creates a matrix of shape (input_num, output_num) for w and initializes b. \n",
    "    \n",
    "    Argument:\n",
    "    input_num -- previous layer's nodes\n",
    "    output_num -- next layer's nodes\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized matrix of shape (input_num, output_num)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros((input_num, output_num))\n",
    "    b = np.zeros((output_num, 1))\n",
    "    \n",
    "    epsilon_init = np.sqrt(6)/np.sqrt(input_num + output_num)\n",
    "    w = np.random.rand(input_num, output_num) * 2 * epsilon_init - epsilon_init\n",
    "    b = np.random.rand(output_num , 1) * 2 * epsilon_init - epsilon_init\n",
    "    ### END CODE HERE ###\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias\n",
    "    X -- data\n",
    "    Y -- true \"label\"\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = np.float(X.shape[1])\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T,X) + b)                                    # compute activation\n",
    "    cost = -(1/m) * (np.sum((Y*np.log(A))+ ((1-Y)*np.log(1-A))))     # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (1/m)*(np.dot(X,(A-Y).T))\n",
    "    db = (1/m)*(np.sum(A-Y, axis=1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias\n",
    "    X -- data \n",
    "    Y -- true \"label\"\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"].reshape(-1,1)\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias\n",
    "    X -- data of size\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a news being present\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = np.argmax(sigmoid(np.dot(w.T,X) + b), axis=0).reshape(1,-1)\n",
    "    ### END CODE HERE ###\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color='blue'>\n",
    "**What to remember:**\n",
    "You've implemented several functions that:\n",
    "- Initialize (w,b)\n",
    "- Optimize the loss iteratively to learn parameters (w,b):\n",
    "    - computing the cost and its gradient \n",
    "    - updating the parameters using gradient descent\n",
    "- Use the learned (w,b) to predict the labels for a given set of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set\n",
    "    Y_train -- training labels \n",
    "    X_test -- test set\n",
    "    Y_test -- test labels\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    oneHotEncoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "    train_y_en = oneHotEncoder.fit_transform(Y_train.reshape(-1,1)).T\n",
    "    test_y_en = oneHotEncoder.fit_transform(Y_test.reshape(-1,1)).T\n",
    "    \n",
    "    input_num = X_train.shape[0]\n",
    "    output_num = train_y_en.shape[0]\n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(input_num, output_num)\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, train_y_en, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.462307\n",
      "Cost after iteration 100: 2.202362\n",
      "Cost after iteration 200: 1.958701\n",
      "Cost after iteration 300: 1.758287\n",
      "Cost after iteration 400: 1.593217\n",
      "Cost after iteration 500: 1.456272\n",
      "Cost after iteration 600: 1.341510\n",
      "Cost after iteration 700: 1.244276\n",
      "Cost after iteration 800: 1.160991\n",
      "Cost after iteration 900: 1.088920\n",
      "Cost after iteration 1000: 1.025960\n",
      "Cost after iteration 1100: 0.970484\n",
      "Cost after iteration 1200: 0.921220\n",
      "Cost after iteration 1300: 0.877164\n",
      "Cost after iteration 1400: 0.837515\n",
      "Cost after iteration 1500: 0.801626\n",
      "Cost after iteration 1600: 0.768972\n",
      "Cost after iteration 1700: 0.739119\n",
      "Cost after iteration 1800: 0.711710\n",
      "Cost after iteration 1900: 0.686446\n",
      "Cost after iteration 2000: 0.663074\n",
      "Cost after iteration 2100: 0.641381\n",
      "Cost after iteration 2200: 0.621185\n",
      "Cost after iteration 2300: 0.602329\n",
      "Cost after iteration 2400: 0.584679\n",
      "Cost after iteration 2500: 0.568117\n",
      "Cost after iteration 2600: 0.552541\n",
      "Cost after iteration 2700: 0.537861\n",
      "Cost after iteration 2800: 0.523999\n",
      "Cost after iteration 2900: 0.510884\n",
      "train accuracy: 98.11996418979409 %\n",
      "test accuracy: 93.02949061662198 %\n"
     ]
    }
   ],
   "source": [
    "d = model(xtrain_tfidf, train_y, xtest_tfidf, test_y, num_iterations = 3000, learning_rate = .5, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the cost function and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxcdb3/8dcn+740SZckXWnL0tIWWlorW1VEQBQQRFRE0fuoICp69af3+vtdRbwqct1AEC4gFBRRuIAUpLJd9qXQ1rbQfaFLuqRp2ibN3jSf3x/nBKZpti6TyWTez8fjPHLmnDNnPmemnfec7fs1d0dERBJbUqwLEBGR2FMYiIiIwkBERBQGIiKCwkBERFAYiIgICgNJQGZ2upmtinUdIv2JwkD6lJltMLOzYlmDu7/s7sfGsoZ2ZjbLzCr66LU+YmYrzazBzJ43s5HdLPuCmTWZWV04KDwHOIWBDDhmlhzrGgAs0C/+j5lZMfAI8B/AIGAB8NcenvZ1d88Jh34RnhI9/eIfqoiZJZnZv5nZOjOrNrMHzWxQxPyHzGy7mdWY2UtmNiFi3hwzu83MnjSzeuBD4R7Id81saficv5pZRrj8Ab/Gu1s2nP89M9tmZlvN7F/MzM1sbBfb8YKZ/dTMXgUagDFmdqWZrTCzvWa23sy+Gi6bDcwDSiN+gZf29F4cpk8By9z9IXdvAq4DJpvZcUe4XhkgFAbSX3wTuBA4EygFdgO3RsyfB4wDBgOLgPs7PP9zwE+BXOCVcNqlwDnAaGAS8KVuXr/TZc3sHOBfgbOAsWF9PfkCMDusZSOwAzgfyAOuBH5jZie7ez1wLrA14hf41l68F+8xsxFmtqeb4XPhohOAJe3PC197XTi9Kz83s51m9qqZzerFdkscS4l1ASKhrxIclqgAMLPrgE1m9gV3b3X3u9sXDOftNrN8d68JJz/m7q+G401mBnBz+OWKmT0OTOnm9bta9lLgHndfFs77MXB5D9syp3350N8jxl80s6eB0wlCrTPdvheRC7r7JqCgh3oAcoCqDtNqCAKrM98HlgMtwGXA42Y2xd3X9eK1JA5pz0D6i5HAo+2/aIEVwH5giJklm9kN4WGTWmBD+JziiOdv7mSd2yPGGwi+ELvS1bKlHdbd2et0dMAyZnaumb1hZrvCbTuPA2vvqMv3ohev3ZU6gj2TSHnA3s4Wdvf57r7X3Zvd/V7g1bBuGaAUBtJfbAbOdfeCiCHD3bcQHAK6gOBQTT4wKnyORTw/Ws3vbgPKIx4P78Vz3qvFzNKBh4FfAkPcvQB4kvdr76zu7t6LA4SHieq6GT4fLroMmBzxvGzgmHB6bzgHvt8ywCgMJBZSzSwjYkgBbgd+2n65o5mVmNkF4fK5QDNQDWQBP+vDWh8ErjSz480sC/jhIT4/DUgnOETTambnAmdHzK8EiswsP2Jad+/FAdx9U8T5hs6G9nMrjwITzezi8OT4D4Gl7r6y4zrNrMDMPtb+2YSBcgbw1CFuu8QRhYHEwpNAY8RwHXATMBd42sz2Am8AM8Ll7yM4EbuF4Dj2G31VqLvPA24GngfWAq+Hs5p7+fy9BCeEHyQ4Efw5gu1sn78SeABYHx4WKqX79+Jwt6MKuJjgJPvucH2Xtc83sx+Y2bzwYSrwnwQBthP4BnChu+tegwHM1LmNSO+Z2fHAO0B6x5O5IvFMewYiPTCzi8wszcwKgV8AjysIZKBRGIj07KsEh0zWEVzVc3VsyxE5+nSYSEREtGcgIiJxeAdycXGxjxo1KtZliIjElYULF+5095Ku5sddGIwaNYoFCxbEugwRkbhiZhu7m6/DRCIiojAQERGFgYiIEMUwCNs1edPMlpjZsrDp347LzAo7E1kcDofa7ouIiBwF0TyB3Ax82N3rzCwVeMXM5rl7x3ZlXnb386NYh4iI9CBqYeDB3Wx14cPUcNAdbiIi/VBUzxmEnZIsJuj27xl3n9/JYjPDQ0nzLKJf2w7rmW1mC8xsQVVVx86aRETkSEU1DNx9v7tPIegcZLqZTeywyCJgpLtPBn4H/K2L9dzh7tPcfVpJSZf3THRr1fa93PiPldQ07jus54uIDGR9cjWRu+8BXiDocDxyeq2714XjTxJ0etJdd4CHbWN1Pb9/YR0bq+ujsXoRkbgWzauJSsysIBzPJOiycGWHZYZa2HO5mU0P66mORj3lhVkAVOxujMbqRUTiWjSvJhoG3GtmyQRf8g+6+xNmdhWAu98OXAJcbWatBD1eXeZRaka1rDATgIrdDdFYvYhIXIvm1URLgZM6mX57xPgtwC3RqiFSfmYqeRkp2jMQEelEQt2BXF6YpTAQEelEQoVBWWGmDhOJiHQiocKgvDCTLbsbUe9uIiIHSrAwyKK+ZT97GnSvgYhIpAQLg/YrinTeQEQkUoKGgc4biIhESrAw0I1nIiKdSagwyM9MJTc9RXsGIiIdJFQYQHB56ZY92jMQEYmUcGGgG89ERA6WgGGQSYXuNRAROUBChkFdc6v6NRARiZCAYaArikREOkrAMNC9BiIiHSVwGGjPQESkXcKFQX5mKjnp6tdARCRSwoWBmb13RZGIiAQSLgyg/fJSnTMQEWmXoGGQpX4NREQiJGQYlBVksre5ldrG1liXIiLSLyRkGLRfUbRZh4pERICEDYPgxjM1WCciEkjQMNC9BiIikRIyDAqyUslOS9YVRSIioaiFgZllmNmbZrbEzJaZ2Y87WcbM7GYzW2tmS83s5GjV0+F11ZS1iEiElCiuuxn4sLvXmVkq8IqZzXP3NyKWORcYFw4zgNvCv1FXphvPRETeE7U9Aw/UhQ9Tw6Hjhf0XAPeFy74BFJjZsGjVFEk3nomIvC+q5wzMLNnMFgM7gGfcfX6HRcqAzRGPK8JpHdcz28wWmNmCqqqqo1JbeWEme5vUr4GICEQ5DNx9v7tPAcqB6WY2scMi1tnTOlnPHe4+zd2nlZSUHJXa3ru8VIeKRET65moid98DvACc02FWBTA84nE5sLUvalK/BiIi74vm1UQlZlYQjmcCZwErOyw2F7givKroA0CNu2+LVk2R1OOZiMj7onk10TDgXjNLJgidB939CTO7CsDdbweeBM4D1gINwJVRrOcAhVmpZKYmKwxERIhiGLj7UuCkTqbfHjHuwDXRqqE77/droMNEIiIJeQdyu/LCTLVPJCJCwoeB7kIWEYGED4NMahr3Udukew1EJLEleBjoXgMREUjwMChTU9YiIkCCh4FuPBMRCSR0GBRlp5GRmqTDRCKS8BI6DNSvgYhIIKHDAMKmrPfoMJGIJDaFgTq5ERFRGJQVZLGnYR97da+BiCSwhA+D9iuK1CyFiCQyhUF7GOhQkYgkMIWB+jUQEVEYFOekkZ6SpBvPRCShJXwYvN+vgfYMRCRxJXwYAJTpxjMRSXAKA1CPZyKS8BQGBGGwu2Ef9c2tsS5FRCQmFAZE9Gugew1EJEEpDFBT1iIiCgMiw0B7BiKSmBQGQHF2OmkpSQoDEUlYCgMgKckoL9AVRSKSuBQGoTLdeCYiCSxqYWBmw83seTNbYWbLzOzaTpaZZWY1ZrY4HH4YrXp6Ul6YpcbqRCRhpURx3a3Ad9x9kZnlAgvN7Bl3X95huZfd/fwo1tEr5YWZVNe30NDSSlZaNN8WEZH+J2p7Bu6+zd0XheN7gRVAWbRe70ipKWsRSWR9cs7AzEYBJwHzO5k908yWmNk8M5vQxfNnm9kCM1tQVVUVlRp1eamIJLKoh4GZ5QAPA99y99oOsxcBI919MvA74G+drcPd73D3ae4+raSkJCp1vt+vga4oEpHEE9UwMLNUgiC4390f6Tjf3WvdvS4cfxJINbPiaNbUlZKcdNKSda+BiCSmaF5NZMAfgBXu/usulhkaLoeZTQ/rqY5WTd1JSrLg8lK1TyQiCSial82cCnwBeNvMFofTfgCMAHD324FLgKvNrBVoBC5zd49iTd1SJzcikqiiFgbu/gpgPSxzC3BLtGo4VOWFmTyzvDLWZYiI9DndgRyhrCCTnXUtNLbsj3UpIiJ9SmEQ4f1+DXRFkYgkFoVBhPZ7DTbrvIGIJBiFQYT39gwUBiKSYBQGEQbnppOabLqiSEQSjsIgQlKSUaZ+DUQkASkMOlC/BiKSiBQGHZQXZCkMRCThKAw6KC/MZGddM037dK+BiCQOhUEH5YPCfg3URpGIJBCFQQfvN2WtMBCRxKEw6OD9Tm50RZGIJA6FQQeDczNISdK9BiKSWBQGHSQnGeWFmSzcsJsYtqYtItKnFAaduPLU0by5YReP/nNLrEsREekTCoNOXP6BkZw8ooDrn1jOzrrmWJcjIhJ1CoNOJCcZv7h4Eg3N+/nx48tjXY6ISNQpDLowbkgu13xoLI8v2cpzK9T7mYgMbAqDblw96xjGD8nh//3tHfY27Yt1OSIiUaMw6EZaShI3XDyJ7bVN3PiPVbEuR0QkanoVBmb26d5MG4hOHlHIlz44ij++sZEFG3bFuhwRkajo7Z7Bv/dy2oD03bOPpawgk+8/vFQN2InIgNRtGJjZuWb2O6DMzG6OGOYArX1SYT+QnZ7CTy+ayLqqen7//NpYlyMictT1tGewFVgANAELI4a5wMeiW1r/MuvYwVx0Uhm/f2EdK7fXxrocEZGjqtswcPcl7n4vMNbd7w3H5wJr3X13d881s+Fm9ryZrTCzZWZ2bSfLWLinsdbMlprZyUe0NVH2H+efQF5mKt9/+G32t6mpChEZOHp7zuAZM8szs0HAEuAeM/t1D89pBb7j7scDHwCuMbMTOixzLjAuHGYDt/W+9L43KDuNH33iBJZs3sOc1zbEuhwRkaOmt2GQ7+61wKeAe9x9KnBWd09w923uvigc3wusAMo6LHYBcJ8H3gAKzGzYIW1BH/vk5FI+dGwJv3xqFZt3qZlrERkYehsGKeGX9KXAE4f6ImY2CjgJmN9hVhmwOeJxBQcHBmY228wWmNmCqqqqQ335o8rM+M+LTiTJ4AePvq2WTUVkQOhtGFwPPAWsc/e3zGwMsKY3TzSzHOBh4Fvh3sUBszt5ykHfru5+h7tPc/dpJSUlvSw5esoKMvn+ucfx8pqdPLJILZuKSPzrVRi4+0PuPsndrw4fr3f3i3t6npmlEgTB/e7+SCeLVADDIx6XE1zB1O9dPmMkU0cW8pO/L2d7TVOsyxEROSK9vQO53MweNbMdZlZpZg+bWXkPzzHgD8AKd+/qZPNc4IrwqqIPADXuvu2QtiBGksKWTVta27jmz4toaW2LdUkiIoett4eJ7iH44i4lOKb/eDitO6cCXwA+bGaLw+E8M7vKzK4Kl3kSWA+sBe4EvnaoGxBLYwfncOMlk1i4cTc//buauhaR+JXSy+VK3D3yy3+OmX2ruye4+yt0fk4gchkHrullDf3S+ZNKWbxpD3e98i5TRhRw0Und7jCJiPRLvd0z2Glml5tZcjhcDlRHs7B48v1zj2P66EH8+yNvs2Kb7k4WkfjT2zD4MsFlpduBbcAlwJXRKirepCYnccvnTiIvI5Wr/rSQmkb1fSAi8aW3YfAT4IvuXuLugwnC4bqoVRWHBudmcNvlJ7NldyPfeXAxbWquQkTiSG/DYFJkW0TuvovgJjKJMHXkIP7j/BN4dsUOblXrpiISR3obBklmVtj+IGyjqLcnnxPKFTNHcuGUUn797GpeXB3bu6VFRHqrt2HwK+A1M/uJmV0PvAbcGL2y4peZ8bNPncixQ3K59i//VPtFIhIXensH8n3AxUAlUAV8yt3/GM3C4llWWgq3Xz6V/W3O1fcvVO9oItLv9XbPAHdf7u63uPvv3F13WPVgVHE2v7l0Cu9sqeVHjy2LdTkiIt3qdRjIoTvrhCF848Nj+euCzTzw5qZYlyMi0iWFQZR966zxnD6umB89tozFm/fEuhwRkU4pDKIsOcm4+bKTKMlN5ytz3mJdVV2sSxIROYjCoA8UZqdx31emYwaX3zVfVxiJSL+jMOgjx5TkcN+XZ1Df3Mrn75pPZa36QBCR/kNh0IdOKM3j3i9Pp7qumcvvms+u+pZYlyQiAigM+txJIwq564unsGlXA1fcPZ/aJjVqJyKxpzCIgZnHFHH75VNZtX0vX77nLRpaWmNdkogkOIVBjHzouMHcdNlJLNq0m6/+UXcpi0hsKQxi6LwTh3HjJZN5ec1OvvHAP9m3X/0oi0hsKAxi7JKp5Vx/wQSeWV7Jdx9awn71gyAiMaBmqPuBK2aOor55P7/4x0qy0pL52UUnYtZt99EiIkeVwqCfuHrWMdQ3t3LL82vJTkvh/378eAWCiPQZhUE/8p2zx1Pf0spdr7xLXXMrP7lwIqnJOpInItGnMOhHzIwfnn8COekp/O5/17K1ponff/5kctL1MYlIdOlnZz9jZnzn7GP5xcUn8uranVx6++tqukJEok5h0E995pQR3P2lU9hYXc9Ft77Kqu17Y12SiAxgUQsDM7vbzHaY2TtdzJ9lZjVmtjgcfhitWuLVmeNLePCqmex355LbXuPVtTtjXZKIDFDR3DOYA5zTwzIvu/uUcLg+irXErQml+Tz6tVMpLcjki3e/ycMLK2JdkogMQFELA3d/CdgVrfUnktKCTB66eiYzxgziOw8t4aZn1+Cum9NE5OiJ9TmDmWa2xMzmmdmErhYys9lmtsDMFlRVVfVlff1GXkYq93xpOhefXM5vnl3N9/5nqZqvEJGjJpbXLC4CRrp7nZmdB/wNGNfZgu5+B3AHwLRp0xL2J3FaShK//PQkygszuem5NWyvDS49zc1IjXVpIhLnYrZn4O617l4Xjj8JpJpZcazqiRdmxrc/Op4bL5nE6+uqueCWV1m2tSbWZYlInItZGJjZUAvbWzCz6WEt1bGqJ95cOm04f/qXGdS3tHLRra9x72sbdB5BRA5bNC8tfQB4HTjWzCrM7CtmdpWZXRUucgnwjpktAW4GLnN9mx2SD4wpYt61Z3DauGJ+NHcZs/+4kN3qSlNEDoPF2/fvtGnTfMGCBbEuo19xd+5+dQM3zFtBcU46v/3MFGaMKYp1WSLSj5jZQnef1tX8WF9NJEeBmfGV00bzyNWnkp6SxGfvfIPfPrtafSOISK8pDAaQE8vzeeKbp3PhlDJ+++waPnfnG2yraYx1WSISBxQGA0xOegq//swUfvXpyby9pYbzbnqZ51ZUxrosEennFAYD1MVTy3niG6dRWpDJV+5dwHVzl9HYsj/WZYlIP6UwGMDGlOTwyNc+yJWnjmLOaxv46G9e5PmVO2Jdloj0QwqDAS49JZkffWICf539ATJSk7lyzlt87f6F6iNBRA6gMEgQM8YU8eQ3T+f/fOxYnluxg4/86kXmvPqurjgSEUBhkFDSUpK45kNjefrbZ3DyyEKue3w5F976Km9XqDkLkUSnMEhAI4uyuffKU/jdZ09ie20TF9z6CtfNXcbepn2xLk1EYkRhkKDMjE9MLuXZfz2Tz88Yyb2vb+CsX7/IvLe3qY0jkQSkMEhw+Zmp/OTCiTz6tVMpyk7n6vsXccXdb+rQkUiCURgIAFOGFzD366fyw/NP4J0tNXzille45s+LWF9VF+vSRKQPqKE6Ocjepn3c+fK73PXyeppb27h02nCu/cg4huZnxLo0ETlMPTVUpzCQLu2sa+aW/13L/fM3kmTGlz44iqtnHUNBVlqsSxORQ6QwkCO2eVcDv3lmNY8u3kJOegpXnXkMV546iqy0WPaaKiKHQmEgR83K7bX88qnVPLuikuKcdL75kbFcOm04GanJsS5NRHqgMJCjbuHGXfxi3ire3LCL4px0rjx1FJfPGEl+VmqsSxORLigMJCrcndfXVfPfL63nxdVVZKUlc9kpI/jK6aMpK8iMdXki0oHCQKJuxbZa7nxpPXOXbMWBT04uZfYZYzh+WF6sSxORkMJA+syWPY3c/cq7PPDmJhpa9nPG+BKuOmMMM48pwsxiXZ5IQlMYSJ+radjHn+Zv5J5XN7CzrpmJZXl85bTRnDtxmE42i8SIwkBipmnffh795xbufGk963fWU5CVysUnl/PZ6SMYOzgn1uWJJBSFgcRcW5vz+vpq/jx/E08t205rmzNj9CA+N2ME50wcSnqK9hZEoq2nMNBdQxJ1SUnGqWOLOXVsMVV7m3lo4Wb+8uZmrv3LYgqzUrlkarC3MKZEewsisRK1PQMzuxs4H9jh7hM7mW/ATcB5QAPwJXdf1NN6tWcwMLS1Oa+u28mf52/imeWVtLY5M8cU8dkZIzj7hCE6tyBylMVyz2AOcAtwXxfzzwXGhcMM4LbwrySApCTj9HElnD6uhB21TTy0sIIH3tzENx/4JznpKXxswlAuPKmUmWOKSElW47oi0RbVcwZmNgp4oos9g/8GXnD3B8LHq4BZ7r6tu3Vqz2Dgaj+38NjiLcx7ezt7m1spzknnE5OHccGUMiaX5+sSVZHD1J/PGZQBmyMeV4TTDgoDM5sNzAYYMWJEnxQnfS/y3ML1F0zk+ZU7eGzxVu5/YxP3vLqB0cXZfHJyKRdMKdX5BZGjLJZh0NlPvE53U9z9DuAOCPYMolmU9A8Zqcmce+Iwzj1xGDWN+/jHO9t4bPFWbv7fNdz03Bomlefz8ROH8bEJQxlVnB3rckXiXizDoAIYHvG4HNgao1qkH8vPTOUzp4zgM6eMYHtNE48v2cpjS7bw83kr+fm8lYwfksPHJgzl7BOGMrEsT4eSRA5DLM8ZfBz4OsHVRDOAm919ek/r1DkDabd5VwPPLK/kqWXbeWvDLtocSvMzOHvCUM6eMITpowbp5LNIKGY3nZnZA8AsoBioBH4EpAK4++3hpaW3AOcQXFp6pbv3+C2vMJDO7Kpv4dkVlTy9rJKX11TR3NpGQVYqHzluCB89YQinjSsmJ1231Uji0h3IknAaWlp5aXUVTy+r5NkVldQ2tZKabEwdWciZ4wdz5vgSjh+Wq8NJklAUBpLQ9u1v460Nu3hp9U5eXF3Fim21AAzOTeeM8SWcOb6E08cVq19nGfAUBiIRKmubeGl1FS+uruLlNTupadxHksHk4QWcOb6E08YWM6m8gLQUnWuQgUVhINKF/W3Okoo9vLgqCIclFXtwh8zUZKaNKmTmMUXMHFPEiWX5OhEtcU9hINJLu+tbmP9uNa+vq+b19dWsrqwDIDstmVNGD2LmmCJmHlPEhNJ8kpN0vkHiS3++A1mkXynMTuOcicM4Z+IwAHbWNTN//S5eX7+T19dV88KqKgBy01OYPnoQU0cVMm3kICaV56thPYl7CgORLhTnpPPxScP4+KQgHHbUNvHGu7t4fV0189+t5rmVOwBITTYmlOYzbWQhU0cWMnVUIYNzM2JZusgh02EikcO0q76FRRt3s2DjbhZt3M2Sij00t7YBMHxQJtNGDmLqyEKmDC/g2KG5pOq8g8SQzhmI9JGW1jaWba1h4cbdLNgQhMTOumYA0lOSmFCax+ThBUwuL2BSeT6jirJJ0rkH6SMKA5EYcXc272pkccUelmzew9KKPbyzpZbGffsByMtIYVIYDJPKC5g8PJ+heRm6GU6iQieQRWLEzBhRlMWIoiw+ObkUgNb9bazZUcfSij0s3lzD0oo93PHSelrbgh9lRdlpnFCaxwmleUwozeeEYXmMLs7W1UsSddozEImxpn37Wba1lrcr9rB8Wy3LttayunIv+/YH/zez0pI5bmhuEA6leUwozWP8kFxdwSSHRIeJROJQS2sba3bsZfnWIByWb61l+bZa6ppbAUgyGFWczXFDcxk/JJfjhuZy7NA8RgzK0l6EdEqHiUTiUFpKEhNK85lQms+nw2ltbc7m3Q0s21rLym21rKoMwmLeO9tp/02XkZrE+CGRAZHL2ME5OhchPdKegUica2hpZU1lHau272Xl9r2srgz+tl/JBJCTnsIxg3MYW5LD2ME5jBsc/B2uPYmEoT0DkQEuKy0luGR1eMEB06vrmllVuZd1O+pYu6OONTvqeHlNFQ8vqnhvmbSUJMYUZzN2cA7HlOQwpiSb0cXBkJuR2tebIjGkMBAZoIpy0vlgTjofPKb4gOk1jftYVxUERPuwtKKGJ9/eRlvEgYLinHTGFGcfEBBjSrIZPiiL9BSdvB5oFAYiCSY/M5WTRxRy8ojCA6Y3t+5nU3UD63fW8+7Oet6tCv4+u6KSnXUt7y2XZDAsP5NRxVmMGJTNqKIsRhZlMbIom5FFWWSl6WslHulTExEA0lOSGTckl3FDcg+aV9O4jw1hSKzfWc+m6no2VDfw1LLt7KpvOWDZktx0Rg4K7q8YMSiL4YVZDB+UxfBBmQzJzdBd1/2UwkBEepSfmdrpeQmA2qZ9bKpuYGN1Axuq69kU/n19XTWP/nMLkdeopCUnUVaYSXlhZhAQhUFIDC/Moqwwk6LsNF31FCMKAxE5InkZqUwsy2diWf5B85pb97NldyObdzeyeVcDm3c3ULGrkc27G3jn7W3sbth3wPLpKUmUFWRSVpgZ/A3HS8PxofkZavAvShQGIhI16SnJjCnJYUxJTqfz9zbtoyIMiq17GtnSPuxuZMW2Ay+PheB8xeDcDIYVZFCan8mw/AyGFYR/8zMoLcikOCddl8seBoWBiMRMbkYqxw9L5fhheZ3Ob9q3/72Q2BqGxNaaJrbVNLJiWy3PraykaV/bAc9JSTKG5AXhMCQ/g6F5wdA+Piw/g8F56boiqgOFgYj0Wxmp3e9ZuDt7GvaxtaaRbXua2FbbxLY9jWyraWLrnkaWbanhuRUHBwbAoOw0huRlMDQvnSF5GQzOy2BwbjqDc9sfp1Ock54wh6UUBiISt8yMwuw0CrPTmFB68DkLCAKjtrGV7bVNbK9torKm6aDxt7fUUl3fTMcGGcyClmQH5wbhEIRFBiW56e8POcHf7PT4/jqNavVmdg5wE5AM3OXuN3SYPwt4DHg3nPSIu18fzZpEJLGYGflZqeRnpXLs0IMvm23Xur+N6voWKmub2FHbTOXe4O+O9/42s3xrLdX1LexvO7gZn6y05APCoTgnnaKcNIpz0il+728wLSc9pd9dNRW1MDCzZOBW4KNABfCWmc119+UdFn3Z3c+PVh0iIr2RkpzEkLwMhuR13391W5uzu6GFqrpmqr9schgAAAnZSURBVPZ2GMJpa3bU8fr6avZ0uFqqXXpK0nshUZSTTlF2GoNy0ijOTmdQdhpFOWkUZQfBMSg7rU+aK4/mnsF0YK27rwcws78AFwAdw0BEJG4kJVnwBZ6TznFDu1923/42dtW3ULW3mer6Fnbubaa6vpmddcH4zvoWttc0hXscze/1YdFRdloyRTnpXDFzJP9y+pgobFV0w6AM2BzxuAKY0clyM81sCbAV+K67L4tiTSIifSa1l3sbEJzb2NvcSnVdC7vqm6mua6G6voVd9S3srGtmV30LJbnpUas1mmHQ2QGxjrG3CBjp7nVmdh7wN2DcQSsymw3MBhgxYsTRrlNEJObMjLyMVPIyUhldnN3nrx/Na6YqgOERj8sJfv2/x91r3b0uHH8SSDWzA5tYDObd4e7T3H1aSUlJFEsWEUlM0QyDt4BxZjbazNKAy4C5kQuY2VALT6mb2fSwnuoo1iQiIp2I2mEid281s68DTxFcWnq3uy8zs6vC+bcDlwBXm1kr0Ahc5vHW9ZqIyACgbi9FRBJAT91eJsZ91iIi0i2FgYiIKAxERERhICIixOEJZDOrAjYe5tOLgZ1HsZz+YKBt00DbHhh42zTQtgcG3jZ1tj0j3b3LG7XiLgyOhJkt6O5sejwaaNs00LYHBt42DbTtgYG3TYezPTpMJCIiCgMREUm8MLgj1gVEwUDbpoG2PTDwtmmgbQ8MvG065O1JqHMGIiLSuUTbMxARkU4oDEREJHHCwMzOMbNVZrbWzP4t1vUcDWa2wczeNrPFZhZ3rfeZ2d1mtsPM3omYNsjMnjGzNeHfwljWeKi62KbrzGxL+DktDjtyigtmNtzMnjezFWa2zMyuDafH5efUzfbE82eUYWZvmtmScJt+HE4/pM8oIc4ZmFkysBr4KEGnO28Bn3X3uO6P2cw2ANPcPS5vljGzM4A64D53nxhOuxHY5e43hKFd6O7fj2Wdh6KLbboOqHP3X8aytsNhZsOAYe6+yMxygYXAhcCXiMPPqZvtuZT4/YwMyA57jEwFXgGuBT7FIXxGibJnMB1Y6+7r3b0F+AtwQYxrSnju/hKwq8PkC4B7w/F7Cf6jxo0utiluufs2d18Uju8FVhD0bx6Xn1M32xO3PFAXPkwNB+cQP6NECYMyYHPE4wri/B9AyIGnzWxh2E/0QDDE3bdB8B8XGBzjeo6Wr5vZ0vAwUlwcUunIzEYBJwHzGQCfU4ftgTj+jMws2cwWAzuAZ9z9kD+jRAkD62TaQDg+dqq7nwycC1wTHqKQ/uc24BhgCrAN+FVsyzl0ZpYDPAx8y91rY13Pkepke+L6M3L3/e4+haCv+elmNvFQ15EoYVABDI94XA5sjVEtR427bw3/7gAeJTgcFu8qw+O67cd3d8S4niPm7pXhf9Y24E7i7HMKj0M/DNzv7o+Ek+P2c+pse+L9M2rn7nuAF4BzOMTPKFHC4C1gnJmNNrM04DJgboxrOiJmlh2eAMPMsoGzgXe6f1ZcmAt8MRz/IvBYDGs5Ktr/Q4YuIo4+p/Dk5B+AFe7+64hZcfk5dbU9cf4ZlZhZQTieCZwFrOQQP6OEuJoIILxU7LdAMnC3u/80xiUdETMbQ7A3AJAC/DnetsnMHgBmETS3Wwn8CPgb8CAwAtgEfNrd4+aEbBfbNIvg8IMDG4Cvth/L7e/M7DTgZeBtoC2c/AOC4+xx9zl1sz2fJX4/o0kEJ4iTCX7gP+ju15tZEYfwGSVMGIiISNcS5TCRiIh0Q2EgIiIKAxERURiIiAgKAxERQWEgUWJmr4V/R5nZ547yun/Q2WtFi5ldaGY/jNK663pe6rDWO8vMnjjCdWwws+Ju5v/FzMYdyWtI/6EwkKhw9w+Go6OAQwqDsJXZ7hwQBhGvFS3fA35/pCvpxXZFnZmlHMXV3Ubw3sgAoDCQqIj4xXsDcHrYRvy3wwa1/svM3gobBftquPyssJ35PxPcEISZ/S1shG9Ze0N8ZnYDkBmu7/7I17LAf5nZOxb08/CZiHW/YGb/Y2Yrzez+8E5UzOwGM1se1nJQ88VmNh5obm8m3MzmmNntZvayma02s/PD6b3erk5e46cWtEX/hpkNiXidSzq+nz1syznhtFcImi9uf+51ZnaHmT0N3BfesfpwWOtbZnZquFyRmT1tZv80s/8mbNMrvNv972GN77S/rwQ3b511lANGYsXdNWg46gNB2/AQ3H37RMT02cD/C8fTgQXA6HC5emB0xLKDwr+ZBM0DFEWuu5PXuhh4huBOzCEEd10OC9ddQ9AmVRLwOnAaMAhYxfs3XxZ0sh1XAr+KeDwH+Ee4nnEE7V5lHMp2dVi/A58Ix2+MWMcc4JIu3s/OtiWDoGXecQRf4g+2v+/AdQTt9meGj/8MnBaOjyBomgHgZuCH4fjHw9qKw/f1zoha8iPGnwGmxvrfm4YjH7RnIH3tbOAKC5rbnQ8UEXyBAbzp7u9GLPtNM1sCvEHQ0GBPx6dPAx7woMGxSuBF4JSIdVd40BDZYoLDV7VAE3CXmX0KaOhkncOAqg7THnT3NndfA6wHjjvE7YrUArQf218Y1tWTzrblOOBdd1/jwbf0nzo8Z667N4bjZwG3hLXOBfLCdq7OaH+eu/8d2B0u/zbBHsAvzOx0d6+JWO8OoLQXNUs/p9076WsGfMPdnzpgotksgl/QkY/PAma6e4OZvUDw67endXelOWJ8P5Di7q1mNh34CEHjhV8HPtzheY1AfodpHdtwcXq5XZ3YF355v1dXON5KeBg3PAyU1t22dFFXpMgakgje18bIBcKjTQetw91Xm9lU4Dzg52b2tLtfH87OIHiPJM5pz0CibS+QG/H4KeBqC5oRxszGW9Dqakf5wO4wCI4DPhAxb1/78zt4CfhMePy+hOCX7ptdFWZBm/b57v4k8C2Chso6WgGM7TDt02aWZGbHAGMIDjX1drt6awMwNRy/gKD3qu6sBEaHNUHQ8FpXniYIPgDMrH27XwI+H047FygMx0uBBnf/E/BL4OSIdY0HlvVQm8QB7RlItC0FWsPDPXOAmwgOaywKf/FW0Xl3fP8ArjKzpQRftm9EzLsDWGpmi9z98xHTHwVmAksIfuF+z923h2HSmVzgMTPLIPhl/+1OlnkJ+JWZWcQv+FUEh6CGAFe5e5OZ3dXL7eqtO8Pa3gSeo/u9C8IaZgN/N7OdBP3gdtXByTeBW8P3NiXcxquAHwMPmNmicPs2hcufCPyXmbUB+4CrAcKT3Y0eJ617SvfUaqlID8zsJuBxd3/WzOYQnJj9nxiXFXNm9m2g1t3/EOta5MjpMJFIz34GZMW6iH5oD+93uC5xTnsGIiKiPQMREVEYiIgICgMREUFhICIiKAxERAT4/wmuYrMWc3ynAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Further analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is: 0.01\n",
      "train accuracy: 24.52999104744852 %\n",
      "test accuracy: 19.302949061662204 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 0.1\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(xtrain_tfidf, train_y, xtest_tfidf, test_y, num_iterations = 3000, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
